# Docify Unified Orchestrator v3.0

A **simplified, safe, and reliable** document analysis function that preserves raw content, generates AI-powered titles, and maintains full compatibility with your existing `docify-website`. This is the unified replacement for the old `llm-analyzer-python` and `document-scraper-python` functions.

## 🎯 **Core Innovation**

This function provides a **simplified, safe approach** to document analysis:

### **🔒 Raw Content Preservation**
- **No Dangerous Cleaning**: Saves browserless content exactly as received
- **Complete Transparency**: You can always see what was actually scraped
- **Reliable Storage**: Raw HTML preserved in `scraped_content` field

### **🤖 AI-Powered Intelligence**
- **Smart Titles**: Generates 2-4 word titles using Gemini 2.5 Flash AI
- **Readable Summaries**: Human-friendly summaries up to 200 characters
- **Compatible Blocks**: Exact same JSON format as your existing `llm-analyzer-python`

### **🎯 8-Step Linear Process**
- **Step 1**: Extract document data from trigger
- **Step 2**: Validate environment and API keys
- **Step 3**: Raw browserless content scraping (no modification)
- **Step 4**: Save raw content to database immediately
- **Step 5**: Generate AI-powered 2-4 word title
- **Step 6**: Create analysis using Gemini (same prompt as llm-analyzer-python)
- **Step 7**: Format blocks in exact same JSON structure
- **Step 8**: Final save and mark as completed

### **🛡️ Safety & Compatibility**
- **Zero Data Loss**: Raw content never modified or cleaned
- **100% Compatible**: Same JSON format as existing analyzer
- **Simple Tracking**: Clear tools usage and research context
- **Error Recovery**: Graceful fallbacks at each step

## 🏗️ **Architecture**

### **Single-File Structure (Simplified)**
```
docify-unified-orchestrator/
├── src/
│   └── main.py                    # Single file with 8 clear functions
├── requirements.txt
├── package.json
└── README.md
```

### **8-Step Processing Functions**
- `extract_document_data()` - Step 1: Get document info from trigger
- `validate_environment()` - Step 2: Check API keys and setup
- `scrape_raw_content()` - Step 3: Browserless scraping (no cleaning)
- `save_raw_content()` - Step 4: Save raw HTML to database
- `generate_ai_title()` - Step 5: AI-powered 2-4 word title
- `generate_analysis()` - Step 6: Gemini analysis with same prompt
- `create_compatible_blocks()` - Step 7: Format blocks like llm-analyzer-python
- `final_save_and_complete()` - Step 8: Save results and mark completed

## 🚀 **Key Features**

### **🔒 Safe Content Handling**
- **Raw Content Only**: Never modifies or cleans scraped content
- **Complete Transparency**: You can always see exactly what was scraped
- **Browserless Integration**: Uses same scraping approach as document-scraper-python
- **No Data Loss**: Raw HTML preserved exactly as received

### **🤖 AI-Powered Analysis**
- **Smart Title Generation**: 2-4 word titles generated by Gemini AI
- **Readable Summaries**: Human-friendly summaries up to 200 characters
- **Exact Compatibility**: Same JSON block format as llm-analyzer-python
- **Gemini 2.5 Flash**: High-performance AI model with better availability

### **📊 Database Integration**
- **Consolidated Schema**: Works with your existing database structure
- **Status Tracking**: Clear status progression (pending → analyzing → completed)
- **Error Recovery**: Graceful failure handling with status updates
- **Optional Fields**: Handles missing database fields safely

## 🔧 **Setup & Installation**

### **1. Environment Variables**
```bash
# Required
GEMINI_API_KEY=your_gemini_api_key
BROWSERLESS_API_KEY=your_browserless_api_key  # Optional but recommended
DATABASE_ID=docify_db
DOCUMENTS_COLLECTION_ID=documents_table
APPWRITE_FUNCTION_API_ENDPOINT=https://cloud.appwrite.io/v1
APPWRITE_FUNCTION_PROJECT_ID=your_project_id

# Database field sizes (matches your current schema)
SCRAPED_CONTENT_MAX=99999    # Your current scraped_content field size
ANALYSIS_BLOCKS_MAX=99999    # Your current analysis_blocks field size
TITLE_MAX=255               # Your current title field size
SUMMARY_MAX=2000           # Your current analysis_summary field size
```

### **2. Database Schema Compatibility**
Your function works with your existing database schema:

| Field | Our Usage | Your Size |
|-------|-----------|-----------|
| `scraped_content` | Raw browserless HTML | 99999 |
| `title` | AI-generated 2-4 words | 255 |
| `analysis_summary` | Readable text (≤200 chars) | 2000 |
| `analysis_blocks` | JSON blocks array | 99999 |
| `gemini_tools_used` | Simple tools list | 1000 |
| `research_context` | Research findings | 5000 |
| `status` | Processing status | - |

### **2. Install Dependencies**
```bash
cd functions/docify-unified-orchestrator
pip install -r requirements.txt
```

### **3. Replace Old Function**
```bash
# Backup your existing functions first
cp -r functions/llm-analyzer-python functions/llm-analyzer-python.backup
cp -r functions/document-scraper-python functions/document-scraper-python.backup

# Replace the main.py file
cp functions/docify-unified-orchestrator/src/main_new.py functions/docify-unified-orchestrator/src/main.py

# Deploy the new unified function
appwrite functions create \
  --functionId docify-unified-orchestrator \
  --name "Docify Unified Orchestrator v3.0" \
  --runtime python-3.9 \
  --entrypoint "src/main.py" \
  --events "databases.docify_db.collections.documents_table.documents.*.create"

appwrite functions deploy --functionId docify-unified-orchestrator
```

## 🎯 **How the Simplified Process Works**

### **8-Step Linear Process**

#### **Step 1: Extract Document Data**
```python
# Get document info from Appwrite trigger
document_data = extract_document_data(context)
# Returns: {'document_id': 'xxx', 'url': 'https://...', 'instructions': '...'}
```

#### **Step 2: Validate Environment**
```python
# Check API keys and setup
validate_environment()  # Ensures GEMINI_API_KEY exists
```

#### **Step 3: Raw Browserless Scraping**
```python
# Scrape content without any cleaning/modification
raw_html = scrape_raw_content(url)
# Returns: Raw HTML exactly as received from browserless
```

#### **Step 4: Save Raw Content**
```python
# Save raw HTML to database immediately
save_raw_content(document_id, raw_html)
# Updates status to 'analyzing'
```

#### **Step 5: Generate AI Title**
```python
# Generate 2-4 word title using Gemini
ai_title = generate_ai_title(url, raw_html, instructions)
# Returns: "API Documentation Guide" (2-4 words)
```

#### **Step 6: Create Analysis**
```python
# Use exact same prompt as llm-analyzer-python
analysis = generate_analysis(url, raw_html, instructions, ai_title)
# Returns: {'summary': '...', 'blocks': [...]}
```

#### **Step 7: Format Compatible Blocks**
```python
# Format blocks in exact same JSON structure
blocks_json = create_compatible_blocks(analysis)
# Returns: JSON string compatible with your docify-website
```

#### **Step 8: Final Save & Complete**
```python
# Save all results and mark as completed
final_save_and_complete(document_id, ai_title, analysis, blocks_json)
# Updates: title, analysis_summary, analysis_blocks, status='completed'
```

## 📊 **Analysis Block Types**

| Type | Description | Use Case |
|------|-------------|----------|
| **summary** | Comprehensive overview | Main document summary |
| **key_points** | Important highlights | Critical information |
| **architecture** | System structure | Technical architecture |
| **mermaid** | Visual diagrams | Flowcharts, system diagrams |
| **code** | Code examples | Implementation examples |
| **api_reference** | API documentation | API specifications |
| **guide** | Step-by-step instructions | Tutorials, guides |
| **comparison** | Feature comparisons | Alternative approaches |
| **best_practices** | Recommendations | Guidelines, tips |
| **troubleshooting** | Common issues | Problem solutions |

## ✅ **Database Schema Compatibility**

Your function works perfectly with your current database:

### **Field Mapping**
- ✅ `scraped_content` (99999 chars) ← Raw browserless HTML
- ✅ `title` (255 chars) ← AI-generated 2-4 word title
- ✅ `status` ← Processing status tracking
- ✅ `analysis_summary` (2000 chars) ← Readable summary (≤200 chars)
- ✅ `analysis_blocks` (99999 chars) ← JSON blocks (llm-analyzer-python format)
- ✅ `gemini_tools_used` (1000 chars) ← Simple tools list
- ✅ `research_context` (5000 chars) ← Research findings

### **Status Flow**
```
pending → scraping → analyzing → completed
   ↓         ↓         ↓         ↓
Created  Scraping  Analyzing  Ready
```

## 🧪 **Testing**

### **Quick Test**
```python
# Test the function locally
python src/main_new.py
```

### **Expected Output**
```
🚀 Docify Unified Orchestrator v3.0 loaded successfully
✨ Features: Raw content preservation, AI titles, compatible blocks
🔒 Safe approach: No content cleaning/modification
🤖 AI-powered: 2-4 word titles, readable summaries
🔗 Compatible: Same JSON format as existing analyzer
```

## 🎯 **Success Criteria**

✅ **Raw Content Preserved**: `scraped_content` contains exact browserless HTML
✅ **AI Titles Generated**: `title` field has 2-4 word AI-generated title
✅ **Readable Summaries**: `analysis_summary` is human-readable text ≤200 chars
✅ **Compatible Blocks**: `analysis_blocks` matches llm-analyzer-python JSON format
✅ **Status Tracking**: Clear progression through all status states
✅ **Error Recovery**: Graceful failure handling with status updates

## 🚀 **Deployment Ready**

Your new simplified unified orchestrator is ready for deployment:

1. ✅ **Environment configured** - Uses your existing GEMINI_API_KEY
2. ✅ **Database compatible** - Works with your current schema
3. ✅ **Website compatible** - Same JSON format as llm-analyzer-python
4. ✅ **Safe approach** - No dangerous content cleaning
5. ✅ **Simple process** - 8 clear steps instead of 50+ complex methods

**Ready to deploy and replace your old functions!** 🎉
